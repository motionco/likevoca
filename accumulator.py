#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Data Accumulator - ë°ì´í„° ëˆ„ì ê¸°
_add.csv íŒŒì¼ì˜ ë°ì´í„°ë¥¼ _list.csv íŒŒì¼ì— ëˆ„ì í•©ë‹ˆë‹¤.
"""

import csv
import json
import datetime
from pathlib import Path

# ê¸°ë³¸ ì„¤ì •
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"

def update_transaction_log(added_concepts):
    """JSON íŠ¸ëœì­ì…˜ ë¡œê·¸ ì—…ë°ì´íŠ¸"""
    log_path = DATA_DIR / "data_tracking_log.json"
    
    # í˜„ì¬ ì‹œê°„
    timestamp = datetime.datetime.now().isoformat() + "Z"
    transaction_id = f"TX_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    try:
        # ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ì½ê¸°
        if log_path.exists() and log_path.stat().st_size > 0:
            with open(log_path, "r", encoding="utf-8") as f:
                try:
                    log_data = json.load(f)
                except json.JSONDecodeError:
                    # íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±
                    log_data = None
        else:
            log_data = None
            
        if log_data is None:
            # ìƒˆ ë¡œê·¸ êµ¬ì¡° ìƒì„±
            log_data = {
                "metadata": {
                    "created": datetime.datetime.now().strftime("%Y-%m-%d"),
                    "last_updated": timestamp,
                    "system_version": "v2.0",
                    "description": "LikeVoca ë°ì´í„° íŠ¸ëœì­ì…˜ ë¡œê·¸"
                },
                "transactions": [],
                "current_status": {}
            }
        
        # ì‹¤ì œ concepts_template_list.csv íŒŒì¼ì˜ ë°ì´í„° ê°œìˆ˜ ê³„ì‚°
        list_file_path = DATA_DIR / "concepts_template_list.csv"
        actual_records_count = 0
        if list_file_path.exists():
            with open(list_file_path, "r", encoding="utf-8-sig") as f:
                reader = csv.reader(f)
                next(reader, None)  # í—¤ë” ìŠ¤í‚µ
                actual_records_count = sum(1 for row in reader)
        
        # ìƒˆ íŠ¸ëœì­ì…˜ ì¶”ê°€
        new_transaction = {
            "transaction_id": transaction_id,
            "timestamp": timestamp,
            "operation": "ADD",
            "added_concepts": added_concepts,
            "summary": {
                "concepts_added": len(added_concepts),
                "total_records_after": actual_records_count
            }
        }
        
        log_data["transactions"].append(new_transaction)
        log_data["metadata"]["last_updated"] = timestamp
        
        # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        log_data["current_status"] = {
            "total_transactions": len(log_data["transactions"]),
            "last_transaction": transaction_id,
            "last_update": timestamp
        }
        
        # ë¡œê·¸ íŒŒì¼ ì €ì¥
        with open(log_path, "w", encoding="utf-8") as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š íŠ¸ëœì­ì…˜ ë¡œê·¸ ì—…ë°ì´íŠ¸: {transaction_id}")
        
    except Exception as e:
        print(f"âš ï¸ íŠ¸ëœì­ì…˜ ë¡œê·¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def identify_skipped_concepts(file_info):
    """1ë‹¨ê³„: concepts íŒŒì¼ì—ì„œ ìŠ¤í‚µ ëŒ€ìƒ concept_id ì‹ë³„"""
    add_file, list_file = file_info
    add_path = DATA_DIR / add_file
    list_path = DATA_DIR / list_file
    
    skipped_concept_ids = set()
    
    if not add_path.exists():
        print(f"âŒ {add_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return skipped_concept_ids
    
    # ê¸°ì¡´ concepts ë°ì´í„° ì½ê¸°
    existing_concept_ids = set()
    existing_word_meanings = set()
    
    if list_path.exists():
        with open(list_path, 'r', encoding='utf-8-sig', newline='') as f:
            reader = csv.DictReader(f)
            for row in reader:
                concept_id = row.get('concept_id', '')
                if concept_id:
                    existing_concept_ids.add(concept_id)
                    
                    # ë‹¨ì–´+ì˜ë¯¸ ì¡°í•©ë„ ì²´í¬
                    english_word = row.get('english_word', '')
                    korean_word = row.get('korean_word', '')
                    if english_word and korean_word and concept_id:
                        parts = concept_id.split('_')
                        meaning = parts[-1] if len(parts) >= 3 else 'unknown'
                        en_combination = f"{english_word}_{meaning}"
                        ko_combination = f"{korean_word}_{meaning}"
                        existing_word_meanings.add(en_combination)
                        existing_word_meanings.add(ko_combination)
    
    # ì‹ ê·œ concepts ë°ì´í„°ì—ì„œ ìŠ¤í‚µ ëŒ€ìƒ ì‹ë³„
    with open(add_path, 'r', encoding='utf-8-sig', newline='') as f:
        reader = csv.DictReader(f)
        new_data = list(reader)
    
    print(f"ğŸ” ìŠ¤í‚µ ëŒ€ìƒ ì‹ë³„: {len(new_data)}ê°œ ì‹ ê·œ ë°ì´í„°, {len(existing_concept_ids)}ê°œ ê¸°ì¡´ concept_id, {len(existing_word_meanings)}ê°œ ê¸°ì¡´ ë‹¨ì–´+ì˜ë¯¸")
    
    for row in new_data:
        concept_id = row.get('concept_id', '')
        skip_reason = None
        
        # 1. concept_id ì¤‘ë³µ ê²€ì‚¬
        if concept_id in existing_concept_ids:
            skip_reason = f"concept_id ì¤‘ë³µ: {concept_id}"
            skipped_concept_ids.add(concept_id)
        
        # 2. ë‹¨ì–´+ì˜ë¯¸ ì¡°í•© ì¤‘ë³µ ê²€ì‚¬
        elif concept_id:
            english_word = row.get('english_word', '')
            korean_word = row.get('korean_word', '')
            if english_word and korean_word:
                parts = concept_id.split('_')
                meaning = parts[-1] if len(parts) >= 3 else 'unknown'
                en_combination = f"{english_word}_{meaning}"
                ko_combination = f"{korean_word}_{meaning}"
                
                if en_combination in existing_word_meanings or ko_combination in existing_word_meanings:
                    skip_reason = f"ë‹¨ì–´+ì˜ë¯¸ ì¤‘ë³µ: {en_combination} ë˜ëŠ” {ko_combination}"
                    skipped_concept_ids.add(concept_id)
        
        if skip_reason:
            print(f"  âš ï¸ ìŠ¤í‚µ ëŒ€ìƒ ì‹ë³„: {concept_id} ({skip_reason})")
    
    return skipped_concept_ids


def accumulate_data():
    """_add.csv íŒŒì¼ë“¤ì˜ ë°ì´í„°ë¥¼ _list.csv íŒŒì¼ë“¤ì— ëˆ„ì  (2ë‹¨ê³„ ë°©ì‹)"""
    file_pairs = [
        ("concepts_template_add.csv", "concepts_template_list.csv"),
        ("examples_template_add.csv", "examples_template_list.csv"), 
        ("grammar_template_add.csv", "grammar_template_list.csv")
    ]
    
    print("ğŸ“ ë°ì´í„° ëˆ„ì  ì‹œì‘...")
    
    # 1ë‹¨ê³„: concepts íŒŒì¼ì—ì„œ ìŠ¤í‚µ ëŒ€ìƒ ì‹ë³„
    print("\nğŸ” 1ë‹¨ê³„: Concepts íŒŒì¼ì—ì„œ ìŠ¤í‚µ ëŒ€ìƒ ì‹ë³„...")
    skipped_concept_ids = identify_skipped_concepts(file_pairs[0])
    print(f"ğŸ“‹ ì´ {len(skipped_concept_ids)}ê°œ concept_idê°€ ìŠ¤í‚µ ëŒ€ìƒìœ¼ë¡œ ì‹ë³„ë¨")
    
    # 2ë‹¨ê³„: ëª¨ë“  íŒŒì¼ì—ì„œ ìŠ¤í‚µ ëŒ€ìƒ ì œì™¸í•˜ê³  ì²˜ë¦¬
    print(f"\nğŸ”§ 2ë‹¨ê³„: ëª¨ë“  íŒŒì¼ì—ì„œ ìŠ¤í‚µ ëŒ€ìƒ ì œì™¸ ì²˜ë¦¬...")
    total_added_concepts = []
    
    for add_file, list_file in file_pairs:
        add_path = DATA_DIR / add_file
        list_path = DATA_DIR / list_file
        
        if not add_path.exists():
            print(f"âš ï¸ {add_file} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue
        
        try:
            # _add.csv íŒŒì¼ ì½ê¸° (BOM ì œê±°ë¥¼ ìœ„í•´ utf-8-sig ì‚¬ìš©)
            with open(add_path, "r", encoding="utf-8-sig") as f:
                reader = csv.DictReader(f)
                new_data = list(reader)
            
            if not new_data:
                print(f"âš ï¸ {add_file}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ê¸°ì¡´ _list.csv íŒŒì¼ ì½ê¸° (ìˆë‹¤ë©´)
            existing_data = []
            existing_concept_ids = set()
            existing_word_meanings = set()  # ë‹¨ì–´+ì˜ë¯¸ ì¡°í•© ì¶”ì 
            list_fieldnames = None
            
            if list_path.exists() and list_path.stat().st_size > 0:
                with open(list_path, "r", encoding="utf-8-sig") as f:
                    reader = csv.DictReader(f)
                    list_fieldnames = reader.fieldnames  # ê¸°ì¡´ íŒŒì¼ì˜ í•„ë“œëª… ì €ì¥
                    # BOM ì œê±°
                    if list_fieldnames and list_fieldnames[0].startswith('\ufeff'):
                        list_fieldnames = [list_fieldnames[0][1:]] + list(list_fieldnames[1:])
                    existing_data = list(reader)
                    
                    # ê¸°ì¡´ ë°ì´í„°ì˜ concept_idì™€ ë‹¨ì–´+ì˜ë¯¸ ì¡°í•© ìˆ˜ì§‘
                    for row in existing_data:
                        concept_id = row.get('concept_id', '')
                        if concept_id:
                            existing_concept_ids.add(concept_id)
                            
                            # concepts íŒŒì¼ì¸ ê²½ìš° ë‹¨ì–´+ì˜ë¯¸ ì¡°í•©ë„ ì²´í¬
                            if add_file == "concepts_template_add.csv":
                                english_word = row.get('english_word', '')
                                korean_word = row.get('korean_word', '')
                                if english_word and korean_word and concept_id:
                                    parts = concept_id.split('_')
                                    meaning = parts[-1] if len(parts) >= 3 else 'unknown'
                                    en_combination = f"{english_word}_{meaning}"
                                    ko_combination = f"{korean_word}_{meaning}"
                                    existing_word_meanings.add(en_combination)
                                    existing_word_meanings.add(ko_combination)
            
            # í•„ë“œëª… ê²°ì •: ê¸°ì¡´ list íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê²ƒ ì‚¬ìš©, ì—†ìœ¼ë©´ add íŒŒì¼ ì‚¬ìš©
            if list_fieldnames is None:
                list_fieldnames = new_data[0].keys() if new_data else []
            
            print(f"ğŸ”§ ì‚¬ìš©í•  í•„ë“œëª… (ì²˜ìŒ 5ê°œ): {list(list_fieldnames)[:5]}")
            print(f"ğŸ”§ ì‹ ê·œ ë°ì´í„° í•„ë“œëª… (ì²˜ìŒ 5ê°œ): {list(new_data[0].keys())[:5] if new_data else []}")
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìƒˆ ë°ì´í„° ì¶”ê°€
            added_count = 0
            print(f"ğŸ” {add_file}: {len(new_data)}ê°œ ì‹ ê·œ ë°ì´í„°, {len(existing_concept_ids)}ê°œ ê¸°ì¡´ concept_id, {len(existing_word_meanings)}ê°œ ê¸°ì¡´ ë‹¨ì–´+ì˜ë¯¸")
            
            for row in new_data:
                concept_id = row.get('concept_id', '')
                skip_reason = None
                
                # 0. 1ë‹¨ê³„ì—ì„œ ì‹ë³„ëœ ìŠ¤í‚µ ëŒ€ìƒ ê²€ì‚¬ (ëª¨ë“  íŒŒì¼ ê³µí†µ)
                if concept_id in skipped_concept_ids:
                    skip_reason = f"1ë‹¨ê³„ì—ì„œ ìŠ¤í‚µëœ concept_id: {concept_id}"
                
                # 1. concept_id ì¤‘ë³µ ê²€ì‚¬ (ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµ)
                elif concept_id in existing_concept_ids:
                    skip_reason = f"concept_id ì¤‘ë³µ: {concept_id}"
                
                # 2. ë‹¨ì–´+ì˜ë¯¸ ì¡°í•© ì¤‘ë³µ ê²€ì‚¬ (concepts íŒŒì¼ì¸ ê²½ìš°ë§Œ ì²´í¬í•˜ì§€ë§Œ ëª¨ë“  íŒŒì¼ì— ì ìš©)
                elif add_file == "concepts_template_add.csv" and concept_id:
                    english_word = row.get('english_word', '')
                    korean_word = row.get('korean_word', '')
                    if english_word and korean_word:
                        parts = concept_id.split('_')
                        meaning = parts[-1] if len(parts) >= 3 else 'unknown'
                        en_combination = f"{english_word}_{meaning}"
                        ko_combination = f"{korean_word}_{meaning}"
                        
                        if en_combination in existing_word_meanings or ko_combination in existing_word_meanings:
                            skip_reason = f"ë‹¨ì–´+ì˜ë¯¸ ì¤‘ë³µ: {en_combination} ë˜ëŠ” {ko_combination}"
                
                # ì¶”ê°€ ë˜ëŠ” ìŠ¤í‚µ
                if concept_id and not skip_reason:
                    existing_data.append(row)
                    existing_concept_ids.add(concept_id)
                    
                    # concepts íŒŒì¼ì¸ ê²½ìš° ë‹¨ì–´+ì˜ë¯¸ ì¡°í•©ë„ ì¶”ê°€
                    if add_file == "concepts_template_add.csv":
                        english_word = row.get('english_word', '')
                        korean_word = row.get('korean_word', '')
                        if english_word and korean_word:
                            parts = concept_id.split('_')
                            meaning = parts[-1] if len(parts) >= 3 else 'unknown'
                            en_combination = f"{english_word}_{meaning}"
                            ko_combination = f"{korean_word}_{meaning}"
                            existing_word_meanings.add(en_combination)
                            existing_word_meanings.add(ko_combination)
                    
                    added_count += 1
                    print(f"  â• ì¶”ê°€: {concept_id}")
                    
                    # concepts íŒŒì¼ì¸ ê²½ìš° íŠ¸ëœì­ì…˜ ë¡œê·¸ìš© ë°ì´í„° ìˆ˜ì§‘
                    if add_file == "concepts_template_add.csv":
                        total_added_concepts.append({
                            "concept_id": concept_id,
                            "domain": row.get('domain', ''),
                            "category": row.get('category', ''),
                            "korean_word": row.get('korean_word', ''),
                            "english_word": row.get('english_word', '')
                        })
                else:
                    print(f"  âš ï¸ ìŠ¤í‚µ: {concept_id} ({skip_reason or 'ë¹ˆ concept_id'})")
            
            print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {added_count}ê°œ ì¶”ê°€ë¨, ì´ {len(existing_data)}ê°œ ë°ì´í„°")
            
            # _list.csv íŒŒì¼ì— ì „ì²´ ë°ì´í„° ì €ì¥ (ë°ì´í„°ê°€ ìˆê±°ë‚˜ ìƒˆë¡œ ì¶”ê°€ëœ ê²½ìš°)
            if existing_data or added_count > 0:
                with open(list_path, "w", encoding="utf-8-sig", newline="") as f:
                    writer = csv.DictWriter(f, fieldnames=list_fieldnames)
                    writer.writeheader()
                    writer.writerows(existing_data)
                
                print(f"âœ… {list_file}: {added_count}ê°œ ì¶”ê°€, ì´ {len(existing_data)}ê°œ")
            else:
                print(f"âš ï¸ {list_file}: ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ {add_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    # íŠ¸ëœì­ì…˜ ë¡œê·¸ ì—…ë°ì´íŠ¸ (conceptsê°€ ì¶”ê°€ëœ ê²½ìš°ì—ë§Œ)
    if total_added_concepts:
        update_transaction_log(total_added_concepts)
    
    print("\nğŸ‰ ë°ì´í„° ëˆ„ì  ì™„ë£Œ!")

def check_data_status():
    """í˜„ì¬ ë°ì´í„° ìƒíƒœ í™•ì¸"""
    print("\nğŸ“Š ë°ì´í„° í˜„í™©:")
    print("-" * 40)
    
    files = [
        "concepts_template_add.csv",
        "concepts_template_list.csv", 
        "examples_template_add.csv",
        "examples_template_list.csv",
        "grammar_template_add.csv",
        "grammar_template_list.csv"
    ]
    
    for file_name in files:
        file_path = DATA_DIR / file_name
        if file_path.exists():
            try:
                with open(file_path, "r", encoding="utf-8-sig") as f:
                    reader = csv.DictReader(f)
                    count = len(list(reader))
                print(f"   ğŸ“„ {file_name}: {count}ê°œ ë°ì´í„°")
            except Exception as e:
                print(f"   âŒ {file_name}: ì½ê¸° ì‹¤íŒ¨ ({e})")
        else:
            print(f"   âšª {file_name}: íŒŒì¼ ì—†ìŒ")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ“¥ Data Accumulator")
    print("=" * 50)
    
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    check_data_status()
    
    # ë°ì´í„° ëˆ„ì 
    accumulate_data()
    
    # ëˆ„ì  í›„ ìƒíƒœ í™•ì¸
    check_data_status()
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1ï¸âƒ£ ë°ì´í„° ë°±ì—…: python backup.py")
    print(f"   2ï¸âƒ£ ìƒˆ ë°ì´í„° ìƒì„±: python generate.py")
    print(f"   3ï¸âƒ£ ë°±ì—… ë³µì›: python restore.py")

if __name__ == "__main__":
    main()
